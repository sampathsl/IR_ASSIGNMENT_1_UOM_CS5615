{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Tokenizing twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Data path\n",
    "\n",
    "inputPath = \"input\"\n",
    "outputPath = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load twitter data file\n",
    "\n",
    "twitterDataPath = os.path.join(inputPath, \"twitter.txt\")\n",
    "inputTwitterFile = open(twitterDataPath, mode='r', encoding=\"Latin-1\")\n",
    "twitterData = inputTwitterFile.read()\n",
    "inputTwitterFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reminds',\n",
       " 'me',\n",
       " 'of',\n",
       " 'Liberal',\n",
       " 'Immigration',\n",
       " 'Fraudster',\n",
       " 'Monsef',\n",
       " 'avoiding',\n",
       " 'deportation',\n",
       " 'from',\n",
       " 'Canada',\n",
       " '.',\n",
       " '#',\n",
       " 'cdnpoli',\n",
       " '#',\n",
       " 'LPC',\n",
       " '#',\n",
       " 'CPCLDRï¿½ï¿½_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/ZOZOSe1CqQ',\n",
       " '#',\n",
       " 'immigration',\n",
       " '#',\n",
       " 'integration',\n",
       " '#',\n",
       " 'canada',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/M5cKGyvV8F',\n",
       " 'We',\n",
       " 'want',\n",
       " 'controlled',\n",
       " 'immigration',\n",
       " 'that',\n",
       " 'contributes',\n",
       " 'positively',\n",
       " 'to',\n",
       " 'the',\n",
       " 'UK',\n",
       " 'economy',\n",
       " '.',\n",
       " 'Same',\n",
       " 'as',\n",
       " 'Australia',\n",
       " '&',\n",
       " 'amp',\n",
       " ';',\n",
       " 'Canada',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/99mYliuOes',\n",
       " 'Is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'Manitoba',\n",
       " 'immigration',\n",
       " 'fee',\n",
       " 'a',\n",
       " 'head',\n",
       " 'tax',\n",
       " '?',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/LsG7C3vLe9',\n",
       " 'Canada',\n",
       " 'immigration',\n",
       " 'profit',\n",
       " 'influence',\n",
       " 'modernistic',\n",
       " 'delhi',\n",
       " 'yet',\n",
       " 'abhinav',\n",
       " ':',\n",
       " 'XKofy',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/becgusY2i6',\n",
       " 'Canada',\n",
       " 'Immigration',\n",
       " 'Minister',\n",
       " 'to',\n",
       " 'ï¿½ï¿½ï¿½Substantially',\n",
       " 'Increase',\n",
       " 'Immigration',\n",
       " 'Numbers',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/nEFw30MRaa',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/cyI867PZRV',\n",
       " 'Mï¿½ï¿½me',\n",
       " 'les',\n",
       " '#',\n",
       " 'USA=pays',\n",
       " \"d'immigration\",\n",
       " 'par',\n",
       " 'excellence',\n",
       " 'CONTRï¿½ï¿½LE',\n",
       " 'RIGOUREUSEMENT',\n",
       " \"l'immigration\",\n",
       " 'et',\n",
       " 'accï¿½ï¿½s',\n",
       " 'ï¿½ï¿½',\n",
       " 'la',\n",
       " '#',\n",
       " 'GreenCARD',\n",
       " '!',\n",
       " 'ï¿½ï¿½_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/IHpVhW2BaG',\n",
       " '@',\n",
       " 'Shawhelp',\n",
       " 'what',\n",
       " 'changes',\n",
       " 'should',\n",
       " 'be',\n",
       " 'made',\n",
       " 'to',\n",
       " 'Canada',\n",
       " \"'s\",\n",
       " 'immigration',\n",
       " 'laws',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'influx',\n",
       " 'of',\n",
       " 'immigration',\n",
       " 'and',\n",
       " 'violence',\n",
       " '?',\n",
       " 'Lï¿½ï¿½immigration',\n",
       " 'irrï¿½ï¿½guliï¿½ï¿½re',\n",
       " 'au',\n",
       " 'Canada',\n",
       " 'dï¿½ï¿½cortiquï¿½ï¿½e',\n",
       " 'en',\n",
       " '5',\n",
       " 'questions',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/f4utO5A7ZF',\n",
       " \"L'immigration\",\n",
       " 'irrï¿½ï¿½guliï¿½ï¿½re',\n",
       " 'au',\n",
       " 'Canada',\n",
       " 'dï¿½ï¿½cortiquï¿½ï¿½e',\n",
       " 'en',\n",
       " '5',\n",
       " 'questions',\n",
       " '-',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/UiBsEZOqas',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/j77dEvjoiX',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/XXDeIG7Dbu',\n",
       " 'Will',\n",
       " 'Media',\n",
       " 'ask',\n",
       " 'the',\n",
       " 'Liberals',\n",
       " 'if',\n",
       " 'they',\n",
       " 'actually',\n",
       " 'have',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'plan',\n",
       " 'for',\n",
       " 'Canada',\n",
       " '_ï¿½ï¿½_ï¿½_',\n",
       " '?',\n",
       " '?',\n",
       " 'From',\n",
       " 'my',\n",
       " 'view',\n",
       " '--',\n",
       " 'immigration',\n",
       " 'out',\n",
       " 'of',\n",
       " 'Cï¿½ï¿½_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/YAgwmZ8ECp',\n",
       " 'Dan',\n",
       " 'Murray',\n",
       " 'ofï¿½ï¿½Immigration',\n",
       " 'Watch',\n",
       " 'Canada',\n",
       " 'is',\n",
       " 'xenophobic',\n",
       " 'racist',\n",
       " 'fear-mongering',\n",
       " 'liar',\n",
       " '#',\n",
       " 'racism',\n",
       " '#',\n",
       " 'canada',\n",
       " '#',\n",
       " 'cdnpoli',\n",
       " '#',\n",
       " 'hatecrimeï¿½ï¿½_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/kwZ3csvYxM',\n",
       " 'Le',\n",
       " 'Canada',\n",
       " 'lance',\n",
       " 'une',\n",
       " 'vaste',\n",
       " 'campagne',\n",
       " \"d'immigration\",\n",
       " 'pour',\n",
       " 'faire',\n",
       " 'face',\n",
       " 'ï¿½ï¿½',\n",
       " 'son',\n",
       " 'besoin',\n",
       " 'de',\n",
       " 'main',\n",
       " 'd',\n",
       " \"'\",\n",
       " 'ï¿½ï¿½uvre',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/kXdfMGTZzN',\n",
       " 'Lï¿½ï¿½',\n",
       " '#',\n",
       " 'immigration',\n",
       " 'irrï¿½ï¿½guliï¿½ï¿½re',\n",
       " 'au',\n",
       " '#',\n",
       " 'Canada',\n",
       " 'dï¿½ï¿½cortiquï¿½ï¿½e',\n",
       " 'en',\n",
       " '5ï¿½ï¿½questions',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/s3hu1OKKIG',\n",
       " '@',\n",
       " 'Canadidly',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'read',\n",
       " 'the',\n",
       " 'Immigration',\n",
       " 'laws',\n",
       " 'of',\n",
       " 'Canada',\n",
       " 'much',\n",
       " 'stricter',\n",
       " 'than',\n",
       " 'the',\n",
       " 'US',\n",
       " 'Canada',\n",
       " 'Immigration',\n",
       " 'Website',\n",
       " 'Traffic',\n",
       " 'Surges',\n",
       " 'And',\n",
       " 'Crashes',\n",
       " 'In',\n",
       " 'Wake',\n",
       " 'Of',\n",
       " 'Trump',\n",
       " '#',\n",
       " 'fasttraffic',\n",
       " ',',\n",
       " '#',\n",
       " 'sitetraffic',\n",
       " ',',\n",
       " '#',\n",
       " 'website',\n",
       " ',',\n",
       " '#',\n",
       " 'traffic',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/zRlJ26jnkC',\n",
       " 'Mr',\n",
       " 'Know-all',\n",
       " 'of',\n",
       " 'Canada',\n",
       " 'Immigration',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/wTQK4QDiKI',\n",
       " 'Move',\n",
       " 'to',\n",
       " 'Canada',\n",
       " '@',\n",
       " 'LadyMadonna___',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'immigration',\n",
       " 'rules',\n",
       " ',',\n",
       " 'you',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " '...',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/5LIEVHO7A4',\n",
       " '#',\n",
       " 'OnThisDay',\n",
       " 'Annette',\n",
       " 'Toft',\n",
       " 'becomes',\n",
       " 'Canada',\n",
       " \"'s\",\n",
       " '2',\n",
       " 'millionth',\n",
       " 'immigrant',\n",
       " 'since',\n",
       " '1945',\n",
       " '.',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'know',\n",
       " 'your',\n",
       " 'family',\n",
       " \"'s\",\n",
       " 'immigration',\n",
       " 'stï¿½ï¿½_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/UvRuw8eR1b',\n",
       " '.',\n",
       " '@',\n",
       " 'TheEconomist',\n",
       " 'profiles',\n",
       " 'Canada',\n",
       " \"'s\",\n",
       " 'open',\n",
       " 'immigration',\n",
       " 'policies',\n",
       " '&',\n",
       " 'amp',\n",
       " ';',\n",
       " 'how',\n",
       " 'they',\n",
       " 'contribute',\n",
       " 'to',\n",
       " 'our',\n",
       " 'economic',\n",
       " 'success',\n",
       " ':',\n",
       " 'ï¿½ï¿½_',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/4K84EE8Y63',\n",
       " 'Hundreds',\n",
       " 'may',\n",
       " 'lose',\n",
       " 'Canadian',\n",
       " 'citizenship',\n",
       " ',',\n",
       " 'resident',\n",
       " 'status',\n",
       " 'because',\n",
       " 'of',\n",
       " 'one',\n",
       " 'corrupt',\n",
       " 'immigration',\n",
       " 'consultant',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/x2IfO0EXI2',\n",
       " 'Immigration',\n",
       " 'for',\n",
       " 'canada',\n",
       " 'without',\n",
       " 'india',\n",
       " ':',\n",
       " 'an',\n",
       " 'compassionate',\n",
       " 'handle',\n",
       " ':',\n",
       " 'deyFy',\n",
       " \"''\",\n",
       " '#',\n",
       " 'Jamaican',\n",
       " '#',\n",
       " 'immigrants',\n",
       " '#',\n",
       " 'Canada',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/vcmfYGadR5',\n",
       " '#',\n",
       " 'statistics',\n",
       " '#',\n",
       " 'immigration',\n",
       " \"''\",\n",
       " 'Mexican',\n",
       " 'visa',\n",
       " 'lift',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'cost',\n",
       " 'Canada',\n",
       " '$',\n",
       " '262M',\n",
       " 'over',\n",
       " 'a',\n",
       " 'decade',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/9i72fRhtij',\n",
       " 'Are',\n",
       " 'people',\n",
       " 'still',\n",
       " 'moving',\n",
       " 'to',\n",
       " '#',\n",
       " 'Canada',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'Oh',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'right',\n",
       " ',',\n",
       " 'they',\n",
       " 'have',\n",
       " 'real',\n",
       " 'immigration',\n",
       " 'laws',\n",
       " 'and',\n",
       " \"it'sï¿½ï¿½_\",\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/0C5OBfmxLG',\n",
       " 'Here',\n",
       " 'are',\n",
       " 'more',\n",
       " 'details',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Richmond',\n",
       " ',',\n",
       " 'B.C',\n",
       " '.',\n",
       " 'Immigration',\n",
       " 'Consultant',\n",
       " 'Sunny',\n",
       " 'Wang',\n",
       " 'who',\n",
       " 'was',\n",
       " 'sentenced',\n",
       " 'to',\n",
       " '7',\n",
       " 'years',\n",
       " 'in',\n",
       " '...',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/YXH5W53srO',\n",
       " 'I',\n",
       " 'added',\n",
       " 'a',\n",
       " 'video',\n",
       " 'to',\n",
       " 'a',\n",
       " '@',\n",
       " 'YouTube',\n",
       " 'playlist',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/CnEyWN40x3',\n",
       " 'Funny',\n",
       " 'Talking',\n",
       " 'of',\n",
       " 'Haryanavi',\n",
       " 'Jat',\n",
       " 'with',\n",
       " 'Canada',\n",
       " 'Immigration',\n",
       " 'Girl',\n",
       " 'Agent',\n",
       " 'Mexicans',\n",
       " 'Can',\n",
       " 'Now',\n",
       " 'Travel',\n",
       " 'Visa-Free',\n",
       " 'To',\n",
       " 'Canada',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/Ec3XHORO2s',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/RQRr5nebcG',\n",
       " 'Lï¿½ï¿½immigration',\n",
       " 'irrï¿½ï¿½guliï¿½ï¿½re',\n",
       " 'au',\n",
       " 'Canada',\n",
       " 'dï¿½ï¿½cortiquï¿½ï¿½e',\n",
       " 'en',\n",
       " '5ï¿½ï¿½questions',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/DkpuKyWmaK',\n",
       " '@',\n",
       " 'SweetnessShawnB',\n",
       " 'Hes',\n",
       " 'the',\n",
       " 'POS',\n",
       " 'that',\n",
       " 'ramped',\n",
       " 'up',\n",
       " 'immigration',\n",
       " 'for',\n",
       " 'Canada',\n",
       " ',',\n",
       " 'among',\n",
       " 'other',\n",
       " 'globalist',\n",
       " 'policies',\n",
       " '.',\n",
       " 'Canada',\n",
       " 'lifted',\n",
       " 'visa',\n",
       " 'requirements',\n",
       " 'to',\n",
       " 'Mexico',\n",
       " 'as',\n",
       " 'of',\n",
       " 'Dec',\n",
       " '1',\n",
       " ',',\n",
       " '2016',\n",
       " '.',\n",
       " 'Thoughts',\n",
       " '?',\n",
       " '#',\n",
       " 'visa',\n",
       " '#',\n",
       " 'immigration',\n",
       " '@',\n",
       " 'HuffingtonPost',\n",
       " 'people',\n",
       " 'Keep',\n",
       " 'praising',\n",
       " 'Canada',\n",
       " 'and',\n",
       " 'Canada',\n",
       " 'has',\n",
       " 'way',\n",
       " 'stricter',\n",
       " 'immigration',\n",
       " 'laws',\n",
       " 'then',\n",
       " 'us',\n",
       " 'they',\n",
       " 'willl',\n",
       " 'boot',\n",
       " 'your',\n",
       " 'liberal',\n",
       " 'American',\n",
       " 'ass']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing twitter data\n",
    "\n",
    "twitterTokens = word_tokenize(twitterData)\n",
    "outputTwitterFile = os.path.join(outputPath, \"tokenization-results\", \"twitter-tokens.txt\")\n",
    "twitterTokensOutputFile = open(outputTwitterFile, mode='w+',encoding=\"Latin-1\")\n",
    "for token in twitterTokens:\n",
    "    twitterTokensOutputFile.write(token + \"\\n\")\n",
    "twitterTokensOutputFile.close()\n",
    "\n",
    "# Display twitter tokens\n",
    "twitterTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tokenizing student course feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing student course feedback data\n",
    "\n",
    "courseFeedbackDataInputFilePath = os.path.join(inputPath, \"student-course-feedback.txt\")\n",
    "courseFeedbackInputFile = open(courseFeedbackDataInputFilePath, mode='r', encoding=\"utf-8\")\n",
    "courseFeedData = courseFeedbackInputFile.read()\n",
    "courseFeedbackInputFile.close()\n",
    "\n",
    "studentFeedbackTokens = word_tokenize(courseFeedData)\n",
    "\n",
    "courseFeedbackDataOutputFilePath = os.path.join(outputPath, \"tokenization-results\", \"student-feedback-tokens.txt\")\n",
    "studentFeedbackTokensOutputFile = open(courseFeedbackDataOutputFilePath, mode='w+', encoding=\"utf-8\")\n",
    "\n",
    "for token in studentFeedbackTokens:\n",
    "    studentFeedbackTokensOutputFile.write(token + \"\\n\")\n",
    "    \n",
    "studentFeedbackTokensOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Tokenizing research paper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing research paper data\n",
    "\n",
    "researchPaperInputFilePath = os.path.join(inputPath, \"research-paper.txt\")\n",
    "researchPaperInputFile = open(researchPaperInputFilePath, mode='r', encoding=\"utf-8\")\n",
    "researchPaperData = researchPaperInputFile.read()\n",
    "researchPaperInputFile .close()\n",
    "\n",
    "researchPaperTokens = word_tokenize(researchPaperData)\n",
    "\n",
    "researchPaperDataOutputFilePath = os.path.join(outputPath, \"tokenization-results\", \"research-paper-tokens.txt\")\n",
    "researchPaperDataOutputFile = open(researchPaperDataOutputFilePath, mode='w+', encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "for token in researchPaperTokens:\n",
    "    researchPaperDataOutputFile.write(token + \"\\n\")\n",
    "    \n",
    "researchPaperDataOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Twitter tokens isolated correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled tokens count =  87\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker(distance = 2)\n",
    "twitterMisspelledTokens = spell.unknown(twitterTokens)\n",
    "\n",
    "print(\"Twitter misspelled tokens count = \", len(twitterMisspelledTokens))\n",
    "\n",
    "twitterWordCorrectionOutPutFilePath = os.path.join(outputPath, \"isolated-word-correction-result\", \"twitter-isolated-correction-tokens.csv\")\n",
    "twitterWordCorrectionMisspelledOutputFile = open(twitterWordCorrectionOutPutFilePath, mode='w+', encoding=\"Latin-1\")\n",
    "twitterWordCorrectionMisspelledOutputFile.write(\"Misspelled Token,Corrected Token\\n\")\n",
    "\n",
    "for token in twitterMisspelledTokens:\n",
    "    twitterWordCorrectionMisspelledOutputFile.write(token + \",\")\n",
    "    twitterWordCorrectionMisspelledOutputFile.write(spell.correction(token) + \"\\n\")\n",
    "\n",
    "twitterWordCorrectionMisspelledOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Student course feedback tokens isolated correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student feedback misspelled tokens count =  14\n"
     ]
    }
   ],
   "source": [
    "studentFeedbackMisspelledTokens = spell.unknown(studentFeedbackTokens)\n",
    "\n",
    "print(\"Student feedback misspelled tokens count = \", len(studentFeedbackMisspelledTokens))\n",
    "\n",
    "courseFeedBackWordCorrectionOutputFilePath = os.path.join(outputPath, \"isolated-word-correction-result\", \"student-feedback-isolated-correction-tokens.csv\")\n",
    "courseFeedBackWordCorrectionOutputFile = open(courseFeedBackWordCorrectionOutputFilePath, mode='w+', encoding=\"utf-8\")\n",
    "courseFeedBackWordCorrectionOutputFile.write(\"Missplled Token,Corrected Token\\n\")\n",
    "\n",
    "for token in studentFeedbackMisspelledTokens:\n",
    "    courseFeedBackWordCorrectionOutputFile.write(token + \",\")\n",
    "    courseFeedBackWordCorrectionOutputFile.write(spell.correction(token) + \"\\n\")\n",
    "\n",
    "courseFeedBackWordCorrectionOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Research Paper tokens isolated correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper misspelled tokens count =  16\n"
     ]
    }
   ],
   "source": [
    "researchPaperMisspelledTokens = spell.unknown(researchPaperTokens)\n",
    "\n",
    "print(\"Research Paper misspelled tokens count = \", len(researchPaperMisspelledTokens))\n",
    "\n",
    "researhPaperWordCorrectionOutputFilePath = os.path.join(outputPath, \"isolated-word-correction-result\", \"research-paper-isolated-correction-tokens.csv\")\n",
    "researhPaperWordCorrectionOutputFile = open(researhPaperWordCorrectionOutputFilePath, mode='w+', encoding=\"utf-8\")\n",
    "researhPaperWordCorrectionOutputFile.write(\"Missplled Token,Corrected Token\\n\")\n",
    "\n",
    "for token in researchPaperMisspelledTokens:\n",
    "    researhPaperWordCorrectionOutputFile.write(token + \",\")\n",
    "    researhPaperWordCorrectionOutputFile.write(spell.correction(token) + \"\\n\")\n",
    "\n",
    "researhPaperWordCorrectionOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context sensitive word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dictionary from the corpus \"big.txt\"\n",
    "\n",
    "#### \"big.txt\" was downloaded from \"How to Write a Spelling Corrector\" by Peter Norvig from http://norvig.com/spell-correct.html. It is a concatenation of public domain book excerpts from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEditDistanceDictionary = 2\n",
    "prefixLength = 7\n",
    "\n",
    "symSpell = SymSpell(maxEditDistanceDictionary, prefixLength)\n",
    "\n",
    "# creating a dictionary using the big.txt\n",
    "dictionaryFilePath = os.path.join(\"dictionary\", \"big.txt\")\n",
    "if not symSpell.create_dictionary(dictionaryFilePath):\n",
    "    print(\"Corpus file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Context sensitive word correction for Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Biwords >> Reminds me\n",
      "Correct Biwords >> me of\n",
      "Correct Biwords >> of Liberal\n",
      "Correct Biwords >> Liberal Immigration\n",
      "Correct Biwords >> from Canada\n",
      "Correct Biwords >> We want\n",
      "Correct Biwords >> want controlled\n",
      "Correct Biwords >> controlled immigration\n",
      "Correct Biwords >> immigration that\n",
      "Correct Biwords >> that contributes\n",
      "Correct Biwords >> contributes positively\n",
      "Correct Biwords >> positively to\n",
      "Correct Biwords >> to the\n",
      "Correct Biwords >> Same as\n",
      "Correct Biwords >> as Australia\n",
      "Correct Biwords >> Is the\n",
      "Correct Biwords >> the new\n",
      "Correct Biwords >> immigration fee\n",
      "Correct Biwords >> fee a\n",
      "Correct Biwords >> a head\n",
      "Correct Biwords >> head tax\n",
      "Correct Biwords >> Canada immigration\n",
      "Correct Biwords >> immigration profit\n",
      "Correct Biwords >> profit influence\n",
      "Correct Biwords >> delhi yet\n",
      "Correct Biwords >> Canada Immigration\n",
      "Correct Biwords >> Immigration Minister\n",
      "Correct Biwords >> Minister to\n",
      "Correct Biwords >> Increase Immigration\n",
      "Correct Biwords >> Immigration Numbers\n",
      "Correct Biwords >> par excellence\n",
      "Correct Biwords >> what changes\n",
      "Correct Biwords >> changes should\n",
      "Correct Biwords >> should be\n",
      "Correct Biwords >> be made\n",
      "Correct Biwords >> made to\n",
      "Correct Biwords >> to Canada\n",
      "Correct Biwords >> 's immigration\n",
      "Correct Biwords >> immigration laws\n",
      "Correct Biwords >> laws due\n",
      "Correct Biwords >> due to\n",
      "Correct Biwords >> to the\n",
      "Correct Biwords >> the influx\n",
      "Correct Biwords >> influx of\n",
      "Correct Biwords >> of immigration\n",
      "Correct Biwords >> immigration and\n",
      "Correct Biwords >> and violence\n",
      "Correct Biwords >> au Canada\n",
      "Correct Biwords >> en 5\n",
      "Correct Biwords >> 5 questions\n",
      "Correct Biwords >> au Canada\n",
      "Correct Biwords >> en 5\n",
      "Correct Biwords >> 5 questions\n",
      "Correct Biwords >> Will Media\n",
      "Correct Biwords >> Media ask\n",
      "Correct Biwords >> ask the\n",
      "Correct Biwords >> the Liberals\n",
      "Correct Biwords >> Liberals if\n",
      "Correct Biwords >> if they\n",
      "Correct Biwords >> they actually\n",
      "Correct Biwords >> actually have\n",
      "Correct Biwords >> have a\n",
      "Correct Biwords >> a solid\n",
      "Correct Biwords >> solid plan\n",
      "Correct Biwords >> plan for\n",
      "Correct Biwords >> for Canada\n",
      "Correct Biwords >> From my\n",
      "Correct Biwords >> my view\n",
      "Correct Biwords >> immigration out\n",
      "Correct Biwords >> out of\n",
      "Correct Biwords >> Dan Murray\n",
      "Correct Biwords >> Watch Canada\n",
      "Correct Biwords >> Canada is\n",
      "Correct Biwords >> Le Canada\n",
      "Correct Biwords >> Canada lance\n",
      "Correct Biwords >> lance une\n",
      "Correct Biwords >> pour faire\n",
      "Correct Biwords >> faire face\n",
      "Correct Biwords >> de main\n",
      "Correct Biwords >> main d\n",
      "Correct Biwords >> read the\n",
      "Correct Biwords >> the Immigration\n",
      "Correct Biwords >> Immigration laws\n",
      "Correct Biwords >> laws of\n",
      "Correct Biwords >> of Canada\n",
      "Correct Biwords >> Canada much\n",
      "Correct Biwords >> much stricter\n",
      "Correct Biwords >> stricter than\n",
      "Correct Biwords >> than the\n",
      "Correct Biwords >> the US\n",
      "Correct Biwords >> US Canada\n",
      "Correct Biwords >> Canada Immigration\n",
      "Correct Biwords >> In Wake\n",
      "Correct Biwords >> Wake Of\n",
      "Correct Biwords >> of Canada\n",
      "Correct Biwords >> Canada Immigration\n",
      "Correct Biwords >> Move to\n",
      "Correct Biwords >> to Canada\n",
      "Correct Biwords >> immigration rules\n",
      "Correct Biwords >> you ca\n",
      "Correct Biwords >> becomes Canada\n",
      "Correct Biwords >> 's 2\n",
      "Correct Biwords >> 2 millionth\n",
      "Correct Biwords >> millionth immigrant\n",
      "Correct Biwords >> immigrant since\n",
      "Correct Biwords >> Do you\n",
      "Correct Biwords >> you know\n",
      "Correct Biwords >> know your\n",
      "Correct Biwords >> your family\n",
      "Correct Biwords >> 's immigration\n",
      "Correct Biwords >> 's open\n",
      "Correct Biwords >> open immigration\n",
      "Correct Biwords >> immigration policies\n",
      "Correct Biwords >> how they\n",
      "Correct Biwords >> they contribute\n",
      "Correct Biwords >> contribute to\n",
      "Correct Biwords >> to our\n",
      "Correct Biwords >> our economic\n",
      "Correct Biwords >> economic success\n",
      "Correct Biwords >> Hundreds may\n",
      "Correct Biwords >> may lose\n",
      "Correct Biwords >> lose Canadian\n",
      "Correct Biwords >> Canadian citizenship\n",
      "Correct Biwords >> resident status\n",
      "Correct Biwords >> status because\n",
      "Correct Biwords >> because of\n",
      "Correct Biwords >> of one\n",
      "Correct Biwords >> one corrupt\n",
      "Correct Biwords >> corrupt immigration\n",
      "Correct Biwords >> immigration consultant\n",
      "Correct Biwords >> Immigration for\n",
      "Correct Biwords >> for canada\n",
      "Correct Biwords >> canada without\n",
      "Correct Biwords >> without india\n",
      "Correct Biwords >> an compassionate\n",
      "Correct Biwords >> compassionate handle\n",
      "Correct Biwords >> lift expected\n",
      "Correct Biwords >> expected to\n",
      "Correct Biwords >> to cost\n",
      "Correct Biwords >> cost Canada\n",
      "Correct Biwords >> over a\n",
      "Correct Biwords >> a decade\n",
      "Correct Biwords >> Are people\n",
      "Correct Biwords >> people still\n",
      "Correct Biwords >> still moving\n",
      "Correct Biwords >> moving to\n",
      "Correct Biwords >> Oh that\n",
      "Correct Biwords >> 's right\n",
      "Correct Biwords >> they have\n",
      "Correct Biwords >> have real\n",
      "Correct Biwords >> real immigration\n",
      "Correct Biwords >> immigration laws\n",
      "Correct Biwords >> laws and\n",
      "Correct Biwords >> Here are\n",
      "Correct Biwords >> are more\n",
      "Correct Biwords >> more details\n",
      "Correct Biwords >> details on\n",
      "Correct Biwords >> on the\n",
      "Correct Biwords >> the Richmond\n",
      "Correct Biwords >> Immigration Consultant\n",
      "Correct Biwords >> Consultant Sunny\n",
      "Correct Biwords >> who was\n",
      "Correct Biwords >> was sentenced\n",
      "Correct Biwords >> sentenced to\n",
      "Correct Biwords >> to 7\n",
      "Correct Biwords >> 7 years\n",
      "Correct Biwords >> years in\n",
      "Correct Biwords >> in ...\n",
      "Correct Biwords >> I added\n",
      "Correct Biwords >> added a\n",
      "Correct Biwords >> a video\n",
      "Correct Biwords >> video to\n",
      "Correct Biwords >> to a\n",
      "Correct Biwords >> Funny Talking\n",
      "Correct Biwords >> Talking of\n",
      "Correct Biwords >> with Canada\n",
      "Correct Biwords >> Canada Immigration\n",
      "Correct Biwords >> Immigration Girl\n",
      "Correct Biwords >> Girl Agent\n",
      "Correct Biwords >> Agent Mexicans\n",
      "Correct Biwords >> Mexicans Can\n",
      "Correct Biwords >> Can Now\n",
      "Correct Biwords >> Now Travel\n",
      "Correct Biwords >> To Canada\n",
      "Correct Biwords >> au Canada\n",
      "Correct Biwords >> up immigration\n",
      "Correct Biwords >> immigration for\n",
      "Correct Biwords >> for Canada\n",
      "Correct Biwords >> among other\n",
      "Correct Biwords >> Canada lifted\n",
      "Correct Biwords >> requirements to\n",
      "Correct Biwords >> to Mexico\n",
      "Correct Biwords >> Mexico as\n",
      "Correct Biwords >> as of\n",
      "Correct Biwords >> of Dec\n",
      "Correct Biwords >> Dec 1\n",
      "Correct Biwords >> people Keep\n",
      "Correct Biwords >> Keep praising\n",
      "Correct Biwords >> praising Canada\n",
      "Correct Biwords >> Canada and\n",
      "Correct Biwords >> and Canada\n",
      "Correct Biwords >> Canada has\n",
      "Correct Biwords >> has way\n",
      "Correct Biwords >> way stricter\n",
      "Correct Biwords >> stricter immigration\n",
      "Correct Biwords >> immigration laws\n",
      "Correct Biwords >> laws then\n",
      "Correct Biwords >> then us\n",
      "Correct Biwords >> us they\n",
      "Correct Biwords >> boot your\n",
      "Correct Biwords >> your liberal\n",
      "Correct Biwords >> liberal American\n",
      "Correct Biwords >> American ass\n"
     ]
    }
   ],
   "source": [
    "twitterContextSensitiveCorrectionOutputFilePath = os.path.join(outputPath, \"context-sensitive-correction-results\", \"twitter-context-sensitive-corrections.csv\")\n",
    "twitterContextSensitiveCorrectionOutputFile = open(twitterContextSensitiveCorrectionOutputFilePath , mode='w+', encoding=\"utf8\")\n",
    "twitterContextSensitiveCorrectionOutputFile.write(\"Incorrect Biword,\" + \"Corrected Result,\" + \"Edit Distance Sum\" + \"\\n\")\n",
    "\n",
    "for index in range(len(twitterTokens) - 1):\n",
    "    biword = twitterTokens[index] + \" \"  + twitterTokens[index + 1]\n",
    "    result = symSpell.word_segmentation(biword)\n",
    "    \n",
    "    if result.corrected_string.lower() != biword.lower():\n",
    "        twitterContextSensitiveCorrectionOutputFile.write(biword + \",\" + result.corrected_string + \",\" + str(result.distance_sum) + \"\\n\")\n",
    "    else:\n",
    "        print(\"Correct Biwords >>\", biword)\n",
    "        \n",
    "twitterContextSensitiveCorrectionOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Context sensitive word correction for student feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Biwords >> Honestly last\n",
      "Correct Biwords >> last seven\n",
      "Correct Biwords >> seven lectures\n",
      "Correct Biwords >> lectures are\n",
      "Correct Biwords >> are good\n",
      "Correct Biwords >> Lectures are\n",
      "Correct Biwords >> are understandable\n",
      "Correct Biwords >> are very\n",
      "Correct Biwords >> very useful\n",
      "Correct Biwords >> useful to\n",
      "Correct Biwords >> The given\n",
      "Correct Biwords >> given opportunity\n",
      "Correct Biwords >> opportunity to\n",
      "Correct Biwords >> to ask\n",
      "Correct Biwords >> ask questions\n",
      "Correct Biwords >> questions from\n",
      "Correct Biwords >> from the\n",
      "Correct Biwords >> the lecturer\n",
      "Correct Biwords >> lecturer is\n",
      "Correct Biwords >> is appreciative\n",
      "Correct Biwords >> please do\n",
      "Correct Biwords >> at class\n",
      "Correct Biwords >> class starting\n",
      "Correct Biwords >> starting it\n",
      "Correct Biwords >> s better\n",
      "Correct Biwords >> better for\n",
      "Correct Biwords >> for us\n",
      "Correct Biwords >> sometimes teaching\n",
      "Correct Biwords >> teaching speed\n",
      "Correct Biwords >> speed is\n",
      "Correct Biwords >> is very\n",
      "Correct Biwords >> very high\n",
      "Correct Biwords >> The lectures\n",
      "Correct Biwords >> lectures are\n",
      "Correct Biwords >> a bit\n",
      "Correct Biwords >> in class\n",
      "Correct Biwords >> class working\n",
      "Correct Biwords >> working activity\n",
      "Correct Biwords >> activity is\n",
      "Correct Biwords >> is a\n",
      "Correct Biwords >> a must\n",
      "Correct Biwords >> please take\n",
      "Correct Biwords >> take another\n",
      "Correct Biwords >> another hour\n",
      "Correct Biwords >> hour in\n",
      "Correct Biwords >> in thursdays\n",
      "Correct Biwords >> We can\n",
      "Correct Biwords >> can hear\n",
      "Correct Biwords >> hear your\n",
      "Correct Biwords >> your voice\n",
      "Correct Biwords >> voice clearly\n",
      "Correct Biwords >> clearly and\n",
      "Correct Biwords >> and can\n",
      "Correct Biwords >> can understand\n",
      "Correct Biwords >> understand the\n",
      "Correct Biwords >> the things\n",
      "Correct Biwords >> things you\n",
      "Correct Biwords >> you teach\n",
      "Correct Biwords >> also good\n",
      "Correct Biwords >> good source\n",
      "Correct Biwords >> source to\n",
      "Correct Biwords >> to refer\n",
      "Correct Biwords >> you can\n",
      "Correct Biwords >> can do\n",
      "Correct Biwords >> do more\n",
      "Correct Biwords >> more example\n",
      "Correct Biwords >> example questions\n",
      "Correct Biwords >> questions within\n",
      "Correct Biwords >> within the\n",
      "Correct Biwords >> the classroom\n",
      "Correct Biwords >> classroom and\n",
      "Correct Biwords >> and it\n",
      "Correct Biwords >> it will\n",
      "Correct Biwords >> will help\n",
      "Correct Biwords >> help us\n",
      "Correct Biwords >> us to\n",
      "Correct Biwords >> to understand\n",
      "Correct Biwords >> understand the\n",
      "Correct Biwords >> the principles\n",
      "Correct Biwords >> principles well\n",
      "Correct Biwords >> Lectures was\n",
      "Correct Biwords >> was well\n",
      "Correct Biwords >> and well\n",
      "Correct Biwords >> well organized\n",
      "Correct Biwords >> It was\n",
      "Correct Biwords >> was easy\n",
      "Correct Biwords >> easy to\n",
      "Correct Biwords >> to understand\n",
      "Correct Biwords >> were also\n",
      "Correct Biwords >> also well\n",
      "Correct Biwords >> well organized\n",
      "Correct Biwords >> Lectures were\n",
      "Correct Biwords >> were good\n",
      "Correct Biwords >> The lecture\n",
      "Correct Biwords >> were well\n",
      "Correct Biwords >> well organized\n",
      "Correct Biwords >> organized and\n",
      "Correct Biwords >> and the\n",
      "Correct Biwords >> the examples\n",
      "Correct Biwords >> examples done\n",
      "Correct Biwords >> done in\n",
      "Correct Biwords >> in the\n",
      "Correct Biwords >> the class\n",
      "Correct Biwords >> class helped\n",
      "Correct Biwords >> helped a\n",
      "Correct Biwords >> a lot\n",
      "Correct Biwords >> lot to\n",
      "Correct Biwords >> to learn\n",
      "Correct Biwords >> learn this\n",
      "Correct Biwords >> this new\n",
      "Correct Biwords >> new language\n",
      "Correct Biwords >> language and\n",
      "Correct Biwords >> and also\n",
      "Correct Biwords >> also the\n",
      "Correct Biwords >> the principles\n",
      "Correct Biwords >> principles of\n",
      "Correct Biwords >> to well\n",
      "Correct Biwords >> Would have\n",
      "Correct Biwords >> have been\n",
      "Correct Biwords >> been better\n",
      "Correct Biwords >> better if\n",
      "Correct Biwords >> if we\n",
      "Correct Biwords >> we discussed\n",
      "Correct Biwords >> discussed more\n",
      "Correct Biwords >> more about\n",
      "Correct Biwords >> about the\n",
      "Correct Biwords >> the solutions\n",
      "Correct Biwords >> solutions of\n",
      "Correct Biwords >> I think\n",
      "Correct Biwords >> think i\n",
      "Correct Biwords >> i learned\n",
      "Correct Biwords >> learned a\n",
      "Correct Biwords >> a lot\n",
      "Correct Biwords >> lot from\n",
      "Correct Biwords >> from the\n",
      "Correct Biwords >> the codes\n",
      "Correct Biwords >> codes you\n",
      "Correct Biwords >> you write\n",
      "Correct Biwords >> write in\n",
      "Correct Biwords >> in the\n",
      "Correct Biwords >> the board\n",
      "Correct Biwords >> When i\n",
      "Correct Biwords >> i compare\n",
      "Correct Biwords >> compare my\n",
      "Correct Biwords >> my codes\n",
      "Correct Biwords >> codes with\n",
      "Correct Biwords >> with yours\n",
      "Correct Biwords >> yours i\n",
      "Correct Biwords >> i can\n",
      "Correct Biwords >> can learn\n",
      "Correct Biwords >> learn about\n",
      "Correct Biwords >> about my\n",
      "Correct Biwords >> my mistakes\n",
      "Correct Biwords >> mistakes and\n",
      "Correct Biwords >> and good\n",
      "Correct Biwords >> practices that\n",
      "Correct Biwords >> that i\n",
      "Correct Biwords >> i should\n",
      "Correct Biwords >> should follow\n",
      "Correct Biwords >> There fore\n",
      "Correct Biwords >> fore i\n",
      "Correct Biwords >> i think\n",
      "Correct Biwords >> think it\n",
      "Correct Biwords >> it would\n",
      "Correct Biwords >> would be\n",
      "Correct Biwords >> be great\n",
      "Correct Biwords >> great if\n",
      "Correct Biwords >> if we\n",
      "Correct Biwords >> we can\n",
      "Correct Biwords >> can discuss\n",
      "Correct Biwords >> discuss more\n",
      "Correct Biwords >> more examples\n",
      "Correct Biwords >> examples in\n",
      "Correct Biwords >> in the\n",
      "Correct Biwords >> the class\n",
      "Correct Biwords >> madam explained\n",
      "Correct Biwords >> explained the\n",
      "Correct Biwords >> concepts clearly\n",
      "Correct Biwords >> clearly with\n",
      "Correct Biwords >> want more\n",
      "Correct Biwords >> more scenario\n",
      "Correct Biwords >> scenario examples\n",
      "Correct Biwords >> examples and\n",
      "Correct Biwords >> and answers\n",
      "Correct Biwords >> answers with\n",
      "Correct Biwords >> with explanations\n",
      "Correct Biwords >> explanations in\n",
      "Correct Biwords >> in future\n",
      "Correct Biwords >> I satisfy\n",
      "Correct Biwords >> satisfy about\n",
      "Correct Biwords >> about first\n",
      "Correct Biwords >> first 7\n",
      "Correct Biwords >> 7 lectures\n",
      "Correct Biwords >> That way\n",
      "Correct Biwords >> way of\n",
      "Correct Biwords >> of teaching\n",
      "Correct Biwords >> teaching is\n",
      "Correct Biwords >> is really\n",
      "Correct Biwords >> really good\n",
      "Correct Biwords >> good for\n",
      "Correct Biwords >> for coming\n",
      "Correct Biwords >> coming lectures\n",
      "Correct Biwords >> lectures too\n",
      "Correct Biwords >> are very\n",
      "Correct Biwords >> very good\n",
      "Correct Biwords >> take good\n",
      "Correct Biwords >> good effort\n",
      "Correct Biwords >> effort to\n",
      "Correct Biwords >> to make\n",
      "Correct Biwords >> every student\n",
      "Correct Biwords >> student in\n",
      "Correct Biwords >> in the\n",
      "Correct Biwords >> the room\n",
      "Correct Biwords >> I was\n",
      "Correct Biwords >> was able\n",
      "Correct Biwords >> able to\n",
      "Correct Biwords >> to obtain\n",
      "Correct Biwords >> obtain a\n",
      "Correct Biwords >> a clear\n",
      "Correct Biwords >> clear picture\n",
      "Correct Biwords >> picture about\n",
      "Correct Biwords >> and its\n",
      "Correct Biwords >> its concepts\n",
      "Correct Biwords >> explanations were\n",
      "Correct Biwords >> were very\n",
      "Correct Biwords >> very clear\n",
      "Correct Biwords >> s very\n",
      "Correct Biwords >> very good\n",
      "Correct Biwords >> good to\n",
      "Correct Biwords >> to letting\n",
      "Correct Biwords >> letting ask\n",
      "Correct Biwords >> ask questions\n",
      "Correct Biwords >> questions and\n",
      "Correct Biwords >> and explain\n",
      "Correct Biwords >> explain again\n",
      "Correct Biwords >> again with\n",
      "Correct Biwords >> with suitable\n",
      "Correct Biwords >> suitable examples\n",
      "Correct Biwords >> some codes\n",
      "Correct Biwords >> codes on\n",
      "Correct Biwords >> on white\n",
      "Correct Biwords >> white board\n",
      "Correct Biwords >> board were\n",
      "Correct Biwords >> were unclear\n",
      "Correct Biwords >> unclear at\n",
      "Correct Biwords >> at the\n",
      "Correct Biwords >> the back\n",
      "Correct Biwords >> overall very\n",
      "Correct Biwords >> very good\n",
      "Correct Biwords >> The lectures\n",
      "Correct Biwords >> lectures were\n",
      "Correct Biwords >> were good\n",
      "Correct Biwords >> good and\n",
      "Correct Biwords >> and clear\n",
      "Correct Biwords >> And they\n",
      "Correct Biwords >> t too\n",
      "Correct Biwords >> too fast\n",
      "Correct Biwords >> Writing code\n",
      "Correct Biwords >> code was\n",
      "Correct Biwords >> was somewhat\n",
      "Correct Biwords >> somewhat confusing\n",
      "Correct Biwords >> confusing because\n",
      "Correct Biwords >> because I\n",
      "Correct Biwords >> t know\n",
      "Correct Biwords >> Actually teaching\n",
      "Correct Biwords >> teaching is\n",
      "Correct Biwords >> is very\n",
      "Correct Biwords >> very good\n",
      "Correct Biwords >> good and\n",
      "Correct Biwords >> and can\n",
      "Correct Biwords >> can understand\n",
      "Correct Biwords >> understand easily\n",
      "Correct Biwords >> easily the\n",
      "Correct Biwords >> the concepts\n",
      "Correct Biwords >> concepts by\n",
      "Correct Biwords >> by examples\n",
      "Correct Biwords >> examples which\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Biwords >> which are\n",
      "Correct Biwords >> are given\n",
      "Correct Biwords >> given in\n",
      "Correct Biwords >> in the\n",
      "Correct Biwords >> will be\n",
      "Correct Biwords >> be more\n",
      "Correct Biwords >> more helpful\n",
      "Correct Biwords >> helpful if\n",
      "Correct Biwords >> if provide\n",
      "Correct Biwords >> provide solved\n",
      "Correct Biwords >> solved questions\n",
      "Correct Biwords >> questions as\n",
      "Correct Biwords >> as well\n"
     ]
    }
   ],
   "source": [
    "studentFeedBackContextSensitiveCorrectionOutputFilePath = os.path.join(outputPath, \"context-sensitive-correction-results\", \"student-feedback-context-sensitive-corrections.csv\")\n",
    "studentFeedBackContextSensitiveCorrectionOutputFile = open(studentFeedBackContextSensitiveCorrectionOutputFilePath, mode='w+', encoding=\"utf-8\")\n",
    "studentFeedBackContextSensitiveCorrectionOutputFile.write(\"Incorrect Biword,\" + \"Corrected Result,\" + \"Edit Distance Sum\" + \"\\n\")\n",
    "\n",
    "for index in range(len(studentFeedbackTokens) - 1):\n",
    "    biword = studentFeedbackTokens[index] + \" \"  + studentFeedbackTokens[index + 1]\n",
    "    result = symSpell.word_segmentation(biword)\n",
    "    \n",
    "    if result.corrected_string.lower() != biword.lower():\n",
    "        studentFeedBackContextSensitiveCorrectionOutputFile.write(biword + \",\" + result.corrected_string + \",\" + str(result.distance_sum) + \"\\n\")\n",
    "    else:\n",
    "        print(\"Correct Biwords >>\", biword)\n",
    "        \n",
    "studentFeedBackContextSensitiveCorrectionOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Research paper context sensitive correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Biwords >> network models\n",
      "Correct Biwords >> models have\n",
      "Correct Biwords >> have shown\n",
      "Correct Biwords >> shown their\n",
      "Correct Biwords >> their promising\n",
      "Correct Biwords >> promising opportunities\n",
      "Correct Biwords >> opportunities for\n",
      "Correct Biwords >> which focus\n",
      "Correct Biwords >> focus on\n",
      "Correct Biwords >> on learning\n",
      "Correct Biwords >> learning the\n",
      "Correct Biwords >> the shared\n",
      "Correct Biwords >> shared layers\n",
      "Correct Biwords >> layers to\n",
      "Correct Biwords >> to extract\n",
      "Correct Biwords >> extract the\n",
      "Correct Biwords >> the common\n",
      "Correct Biwords >> common and\n",
      "Correct Biwords >> in most\n",
      "Correct Biwords >> most existing\n",
      "Correct Biwords >> existing approaches\n",
      "Correct Biwords >> the extracted\n",
      "Correct Biwords >> extracted shared\n",
      "Correct Biwords >> shared features\n",
      "Correct Biwords >> features are\n",
      "Correct Biwords >> are prone\n",
      "Correct Biwords >> prone to\n",
      "Correct Biwords >> to be\n",
      "Correct Biwords >> be contaminated\n",
      "Correct Biwords >> contaminated by\n",
      "Correct Biwords >> features or\n",
      "Correct Biwords >> or the\n",
      "Correct Biwords >> the noise\n",
      "Correct Biwords >> noise brought\n",
      "Correct Biwords >> brought by\n",
      "Correct Biwords >> by other\n",
      "Correct Biwords >> other tasks\n",
      "Correct Biwords >> In this\n",
      "Correct Biwords >> this paper\n",
      "Correct Biwords >> we propose\n",
      "Correct Biwords >> propose an\n",
      "Correct Biwords >> learning framework\n",
      "Correct Biwords >> alleviating the\n",
      "Correct Biwords >> the shared\n",
      "Correct Biwords >> shared and\n",
      "Correct Biwords >> and private\n",
      "Correct Biwords >> private latent\n",
      "Correct Biwords >> latent feature\n",
      "Correct Biwords >> feature spaces\n",
      "Correct Biwords >> spaces from\n",
      "Correct Biwords >> from interfering\n",
      "Correct Biwords >> interfering with\n",
      "Correct Biwords >> with each\n",
      "Correct Biwords >> each other\n",
      "Correct Biwords >> We conduct\n",
      "Correct Biwords >> conduct extensive\n",
      "Correct Biwords >> extensive experiments\n",
      "Correct Biwords >> experiments on\n",
      "Correct Biwords >> on 16\n",
      "Correct Biwords >> 16 different\n",
      "Correct Biwords >> different text\n",
      "Correct Biwords >> text classification\n",
      "Correct Biwords >> classification tasks\n",
      "Correct Biwords >> which demonstrates\n",
      "Correct Biwords >> demonstrates the\n",
      "Correct Biwords >> the benefits\n",
      "Correct Biwords >> benefits of\n",
      "Correct Biwords >> of our\n",
      "Correct Biwords >> our approach\n",
      "Correct Biwords >> we show\n",
      "Correct Biwords >> show that\n",
      "Correct Biwords >> that the\n",
      "Correct Biwords >> the shared\n",
      "Correct Biwords >> shared knowledge\n",
      "Correct Biwords >> knowledge learned\n",
      "Correct Biwords >> learned by\n",
      "Correct Biwords >> by our\n",
      "Correct Biwords >> our proposed\n",
      "Correct Biwords >> proposed model\n",
      "Correct Biwords >> model can\n",
      "Correct Biwords >> can be\n",
      "Correct Biwords >> be regarded\n",
      "Correct Biwords >> regarded as\n",
      "Correct Biwords >> knowledge and\n",
      "Correct Biwords >> and easily\n",
      "Correct Biwords >> easily transferred\n",
      "Correct Biwords >> transferred to\n",
      "Correct Biwords >> to new\n",
      "Correct Biwords >> new tasks\n",
      "Correct Biwords >> learning is\n",
      "Correct Biwords >> is an\n",
      "Correct Biwords >> an effective\n",
      "Correct Biwords >> effective approach\n",
      "Correct Biwords >> approach to\n",
      "Correct Biwords >> to improve\n",
      "Correct Biwords >> improve the\n",
      "Correct Biwords >> the performance\n",
      "Correct Biwords >> performance of\n",
      "Correct Biwords >> of a\n",
      "Correct Biwords >> a single\n",
      "Correct Biwords >> single task\n",
      "Correct Biwords >> task with\n",
      "Correct Biwords >> with the\n",
      "Correct Biwords >> the help\n",
      "Correct Biwords >> help of\n",
      "Correct Biwords >> of other\n",
      "Correct Biwords >> other related\n",
      "Correct Biwords >> related tasks\n",
      "Correct Biwords >> models for\n",
      "Correct Biwords >> learning have\n",
      "Correct Biwords >> have become\n",
      "Correct Biwords >> become very\n",
      "Correct Biwords >> very popular\n",
      "Correct Biwords >> ranging from\n",
      "Correct Biwords >> from computer\n",
      "Correct Biwords >> computer vision\n",
      "Correct Biwords >> to natural\n",
      "Correct Biwords >> natural language\n",
      "Correct Biwords >> language processing\n",
      "Correct Biwords >> since they\n",
      "Correct Biwords >> they provide\n",
      "Correct Biwords >> provide a\n",
      "Correct Biwords >> a convenient\n",
      "Correct Biwords >> convenient way\n",
      "Correct Biwords >> way of\n",
      "Correct Biwords >> of combining\n",
      "Correct Biwords >> combining information\n",
      "Correct Biwords >> information from\n",
      "Correct Biwords >> from multiple\n",
      "Correct Biwords >> multiple tasks\n",
      "Correct Biwords >> most existing\n",
      "Correct Biwords >> existing work\n",
      "Correct Biwords >> work on\n",
      "Correct Biwords >> attempts to\n",
      "Correct Biwords >> to divide\n",
      "Correct Biwords >> divide the\n",
      "Correct Biwords >> the features\n",
      "Correct Biwords >> features of\n",
      "Correct Biwords >> of different\n",
      "Correct Biwords >> different tasks\n",
      "Correct Biwords >> tasks into\n",
      "Correct Biwords >> into private\n",
      "Correct Biwords >> private and\n",
      "Correct Biwords >> and shared\n",
      "Correct Biwords >> shared spaces\n",
      "Correct Biwords >> merely based\n",
      "Correct Biwords >> based on\n",
      "Correct Biwords >> on whether\n",
      "Correct Biwords >> of some\n",
      "Correct Biwords >> some components\n",
      "Correct Biwords >> components should\n",
      "Correct Biwords >> should be\n",
      "Correct Biwords >> be shared\n",
      "Correct Biwords >> As shown\n",
      "Correct Biwords >> shown in\n",
      "Correct Biwords >> in Figure\n",
      "Correct Biwords >> the general\n",
      "Correct Biwords >> model introduces\n",
      "Correct Biwords >> introduces two\n",
      "Correct Biwords >> two feature\n",
      "Correct Biwords >> feature spaces\n",
      "Correct Biwords >> spaces for\n",
      "Correct Biwords >> for any\n",
      "Correct Biwords >> any task\n",
      "Correct Biwords >> one is\n",
      "Correct Biwords >> is used\n",
      "Correct Biwords >> used to\n",
      "Correct Biwords >> to store\n",
      "Correct Biwords >> the other\n",
      "Correct Biwords >> other is\n",
      "Correct Biwords >> is used\n",
      "Correct Biwords >> used to\n",
      "Correct Biwords >> to capture\n",
      "Correct Biwords >> capture shared\n",
      "Correct Biwords >> shared features\n",
      "Correct Biwords >> The major\n",
      "Correct Biwords >> major limitation\n",
      "Correct Biwords >> limitation of\n",
      "Correct Biwords >> of this\n",
      "Correct Biwords >> this framework\n",
      "Correct Biwords >> framework is\n",
      "Correct Biwords >> is that\n",
      "Correct Biwords >> that the\n",
      "Correct Biwords >> the shared\n",
      "Correct Biwords >> shared feature\n",
      "Correct Biwords >> feature space\n",
      "Correct Biwords >> space could\n",
      "Correct Biwords >> could contain\n",
      "Correct Biwords >> contain some\n",
      "Correct Biwords >> some unnecessary\n",
      "Correct Biwords >> while some\n",
      "Correct Biwords >> features could\n",
      "Correct Biwords >> could also\n",
      "Correct Biwords >> also be\n",
      "Correct Biwords >> be mixed\n",
      "Correct Biwords >> mixed in\n",
      "Correct Biwords >> in private\n",
      "Correct Biwords >> private space\n",
      "Correct Biwords >> suffering from\n",
      "Correct Biwords >> from feature\n",
      "Correct Biwords >> feature redundancy\n",
      "Correct Biwords >> Taking the\n",
      "Correct Biwords >> the following\n",
      "Correct Biwords >> following two\n",
      "Correct Biwords >> two sentences\n",
      "Correct Biwords >> sentences as\n",
      "Correct Biwords >> as examples\n",
      "Correct Biwords >> which are\n",
      "Correct Biwords >> are extracted\n",
      "Correct Biwords >> extracted from\n",
      "Correct Biwords >> from two\n",
      "Correct Biwords >> two different\n",
      "Correct Biwords >> different sentiment\n",
      "Correct Biwords >> sentiment classification\n",
      "Correct Biwords >> classification tasks\n",
      "Correct Biwords >> Movie reviews\n",
      "Correct Biwords >> reviews and\n",
      "Correct Biwords >> and Baby\n",
      "Correct Biwords >> Baby products\n",
      "Correct Biwords >> products reviews\n",
      "Correct Biwords >> The infantile\n",
      "Correct Biwords >> infantile cart\n",
      "Correct Biwords >> cart is\n",
      "Correct Biwords >> is simple\n",
      "Correct Biwords >> simple and\n",
      "Correct Biwords >> and easy\n",
      "Correct Biwords >> easy to\n",
      "Correct Biwords >> to use\n",
      "Correct Biwords >> This kind\n",
      "Correct Biwords >> kind of\n",
      "Correct Biwords >> of humour\n",
      "Correct Biwords >> humour is\n",
      "Correct Biwords >> is infantile\n",
      "Correct Biwords >> infantile and\n",
      "Correct Biwords >> and boring\n",
      "Correct Biwords >> The word\n",
      "Correct Biwords >> indicates negative\n",
      "Correct Biwords >> negative sentiment\n",
      "Correct Biwords >> sentiment in\n",
      "Correct Biwords >> in Movie\n",
      "Correct Biwords >> Movie task\n",
      "Correct Biwords >> task while\n",
      "Correct Biwords >> while it\n",
      "Correct Biwords >> it is\n",
      "Correct Biwords >> is neutral\n",
      "Correct Biwords >> neutral in\n",
      "Correct Biwords >> in Baby\n",
      "Correct Biwords >> Baby task\n",
      "Correct Biwords >> the general\n",
      "Correct Biwords >> model could\n",
      "Correct Biwords >> could place\n",
      "Correct Biwords >> place the\n",
      "Correct Biwords >> in a\n",
      "Correct Biwords >> a shared\n",
      "Correct Biwords >> shared space\n",
      "Correct Biwords >> leaving potential\n",
      "Correct Biwords >> potential hazards\n",
      "Correct Biwords >> hazards for\n",
      "Correct Biwords >> for other\n",
      "Correct Biwords >> other tasks\n",
      "Correct Biwords >> the capacity\n",
      "Correct Biwords >> capacity of\n",
      "Correct Biwords >> of shared\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Biwords >> shared space\n",
      "Correct Biwords >> space could\n",
      "Correct Biwords >> could also\n",
      "Correct Biwords >> also be\n",
      "Correct Biwords >> be wasted\n",
      "Correct Biwords >> wasted by\n",
      "Correct Biwords >> by some\n",
      "Correct Biwords >> some unnecessary\n",
      "Correct Biwords >> unnecessary features\n",
      "Correct Biwords >> To address\n",
      "Correct Biwords >> address this\n",
      "Correct Biwords >> this problem\n",
      "Correct Biwords >> in this\n",
      "Correct Biwords >> this paper\n",
      "Correct Biwords >> paper we\n",
      "Correct Biwords >> we propose\n",
      "Correct Biwords >> propose an\n",
      "Correct Biwords >> in which\n",
      "Correct Biwords >> which the\n",
      "Correct Biwords >> the shared\n",
      "Correct Biwords >> shared and\n",
      "Correct Biwords >> and private\n",
      "Correct Biwords >> private feature\n",
      "Correct Biwords >> feature spaces\n",
      "Correct Biwords >> spaces are\n",
      "Correct Biwords >> are in\n",
      "Correct Biwords >> by introducing\n",
      "Correct Biwords >> we design\n",
      "Correct Biwords >> design a\n",
      "Correct Biwords >> shared private\n",
      "Correct Biwords >> private learning\n",
      "Correct Biwords >> learning framework\n",
      "Correct Biwords >> framework to\n",
      "Correct Biwords >> to model\n",
      "Correct Biwords >> model the\n",
      "Correct Biwords >> the text\n",
      "Correct Biwords >> text sequence\n"
     ]
    }
   ],
   "source": [
    "reseachPaperContextSensitiveCorrectionOutputFilePath = os.path.join(outputPath, \"context-sensitive-correction-results\", \"reseach-paper-context-sensitive-corrections.csv\")\n",
    "reseachPaperContextSensitiveCorrectionOutputFile = open(reseachPaperContextSensitiveCorrectionOutputFilePath, mode='w+', encoding=\"utf-8\")\n",
    "reseachPaperContextSensitiveCorrectionOutputFile.write(\"Incorrect Biword,\" + \"Corrected Result,\" + \"Edit Distance Sum\" + \"\\n\")\n",
    "\n",
    "for index in range(len(researchPaperTokens) - 1):\n",
    "    biword = researchPaperTokens[index] + \" \"  + researchPaperTokens[index + 1]\n",
    "    result = symSpell.word_segmentation(biword)\n",
    "    \n",
    "    if result.corrected_string.lower() != biword.lower():\n",
    "        reseachPaperContextSensitiveCorrectionOutputFile.write(biword + \",\" + result.corrected_string + \",\" + str(result.distance_sum) + \"\\n\")\n",
    "    else:\n",
    "        print(\"Correct Biwords >>\", biword)\n",
    "        \n",
    "reseachPaperContextSensitiveCorrectionOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Porter's stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Twitter data stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterStemmed = [stemmer.stem(token) for token in twitterTokens]\n",
    "\n",
    "twitterStemmingOutputFilePath = os.path.join(outputPath, \"stemming-results\", \"twitter-stemmed-results.csv\")\n",
    "twitterStemmingOutputFile = open(twitterStemmingOutputFilePath, mode=\"w+\", encoding=\"utf-8\")\n",
    "twitterStemmingOutputFile.write(\"Original Token,\" + \"Stemmed Token\\n\")\n",
    "\n",
    "for index in range(len(twitterStemmed)):\n",
    "    twitterStemmingOutputFile.write(twitterTokens[index] + \",\" + twitterStemmed[index] + \"\\n\")\n",
    "    \n",
    "twitterStemmingOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Student feed back data stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentFeedbackStemmed = [stemmer.stem(token) for token in studentFeedbackTokens]\n",
    "\n",
    "studentFeedBackStemmingOutputFilePath = os.path.join(outputPath, \"stemming-results\", \"student-feedback-stemmed-results.csv\")\n",
    "studentFeedBackStemmingOutputFile = open(studentFeedBackStemmingOutputFilePath, mode=\"w+\", encoding=\"utf-8\")\n",
    "studentFeedBackStemmingOutputFile.write(\"Original Token,\" + \"Stemmed Token\\n\")\n",
    "\n",
    "for index in range(len(studentFeedbackStemmed)):\n",
    "    studentFeedBackStemmingOutputFile.write(studentFeedbackTokens[index] + \",\" + studentFeedbackStemmed[index] + \"\\n\")\n",
    "    \n",
    "studentFeedBackStemmingOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Research Paper data stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reseachPaperStemmed = [stemmer.stem(token) for token in researchPaperTokens]\n",
    "\n",
    "researchPaperStemmingOutputFilePath = os.path.join(outputPath, \"stemming-results\", \"research-paper-stemmed-results.csv\")\n",
    "researchPaperStemmingOutputFile = open(researchPaperStemmingOutputFilePath, mode=\"w+\", encoding=\"utf-8\")\n",
    "researchPaperStemmingOutputFile.write(\"Original Token,\" + \"Stemmed Token\\n\")\n",
    "\n",
    "for index in range(len(reseachPaperStemmed)):\n",
    "    researchPaperStemmingOutputFile.write(researchPaperTokens[index] + \",\" + reseachPaperStemmed[index] + \"\\n\")\n",
    "    \n",
    "researchPaperStemmingOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lemmatizing twitter feed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterFeedLemmatized = [word## 1) Lemmatizing twitter feed datanet_lemmatizer.lemmatize(word) for word in twitterTokens]\n",
    "\n",
    "twitterFeedLemmatizedOutputFilePath = os.path.join(outputPath, \"lemmatized-results\", \"twitter-feed-lemmatized-results.csv\")\n",
    "twitterFeedLemmatizedOutputFile = open(twitterFeedLemmatizedOutputFilePath, mode=\"w+\", encoding=\"utf-8\")\n",
    "twitterFeedLemmatizedOutputFile.write(\"Original Token,\" + \"Lemmatized Token\\n\")\n",
    "\n",
    "for index in range(len(twitterFeedLemmatized)):\n",
    "    twitterFeedLemmatizedOutputFile.write(twitterTokens[index] + \",\" + twitterFeedLemmatized[index] + \"\\n\")\n",
    "    \n",
    "twitterFeedLemmatizedOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Lemmatizing student feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentFeedbackLemmatized = [wordnet_lemmatizer.lemmatize(word) for word in studentFeedbackTokens]\n",
    "\n",
    "studentFeedBackLemmatizedOutputFilePath = os.path.join(outputPath, \"lemmatized-results\", \"student-feedback-lemmatized-results.csv\")\n",
    "studentFeedBackLemmatizedOutputFile = open(studentFeedBackLemmatizedOutputFilePath, mode=\"w+\", encoding=\"utf-8\")\n",
    "studentFeedBackLemmatizedOutputFile.write(\"Original Token,\" + \"Lemmatized Token\\n\")\n",
    "\n",
    "for index in range(len(studentFeedbackLemmatized)):\n",
    "    studentFeedBackLemmatizedOutputFile.write(studentFeedbackTokens[index] + \",\" + studentFeedbackLemmatized[index] + \"\\n\")\n",
    "    \n",
    "studentFeedBackLemmatizedOutputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Lemmatizing Research paper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "researchPaperLemmatized = [wordnet_lemmatizer.lemmatize(word) for word in researchPaperTokens]\n",
    "\n",
    "researchPaperLemmatizedOutputFilePath = os.path.join(outputPath, \"lemmatized-results\", \"research-paper-lemmatized-results.csv\")\n",
    "researchPaperLemmatizedOutputFile = open(researchPaperLemmatizedOutputFilePath, mode=\"w+\", encoding=\"utf-8\")\n",
    "researchPaperLemmatizedOutputFile.write(\"Original Token,\" + \"Lemmatized Token\\n\")\n",
    "\n",
    "for index in range(len(researchPaperLemmatized)):\n",
    "    researchPaperLemmatizedOutputFile.write(researchPaperTokens[index] + \",\" + researchPaperLemmatized[index] + \"\\n\")\n",
    "    \n",
    "researchPaperLemmatizedOutputFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}